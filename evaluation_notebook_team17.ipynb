{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Notebook for TEAM 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Install your packages below: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxpy\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, you must run your methodology for solving the problem from start to finish :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    \n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv # Imposed Environment\n",
    "import numpy as np\n",
    "\n",
    "## Import your favourite Deep Learning library for RL and other packages here\n",
    "from train_per import *\n",
    "from wrappers import NormalizedMicroGridEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an environment initialization without a Deep RL library, the code can vary depending on which library you \n",
    "use\n",
    "\"\"\"\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i]}) for i in range(3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Training of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load 15\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 30\nbattery_soc_0 0.2\nbattery_power_charge 4\nbattery_power_discharge 4\nbattery_capacity 15\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 0\ngrid_power_import 30\ngrid_power_export 30\nepisode: 0 \tscore: -12233.108345644603 \tmemory length: 5846 \tepsilon: 1.0 \tlearning rate: 0.001\nSimilarity to perfect score 4.006785878246185\nepisode: 1 \tscore: -12587.314144743204 \tmemory length: 11692 \tepsilon: 0.918831875421614 \tlearning rate: 0.001\nSimilarity to perfect score 4.093846416306552\nepisode: 2 \tscore: -10626.391830219725 \tmemory length: 17538 \tepsilon: 0.685943906147323 \tlearning rate: 0.001\nSimilarity to perfect score 3.611869689128604\nload 50\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 75\nbattery_soc_0 0.2\nbattery_power_charge 13\nbattery_power_discharge 13\nbattery_capacity 50\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 0\ngrid_power_import 100\ngrid_power_export 100\nepisode: 0 \tscore: -41270.13346716048 \tmemory length: 5846 \tepsilon: 1.0 \tlearning rate: 0.001\nSimilarity to perfect score 4.041519403693181\nepisode: 1 \tscore: -42163.082208903536 \tmemory length: 11692 \tepsilon: 0.918831875421614 \tlearning rate: 0.001\nSimilarity to perfect score 4.107327790929826\nepisode: 2 \tscore: -35305.60694998984 \tmemory length: 17538 \tepsilon: 0.685943906147323 \tlearning rate: 0.001\nSimilarity to perfect score 3.6019467245727617\nload 38\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 60\nbattery_soc_0 0.2\nbattery_power_charge 10\nbattery_power_discharge 10\nbattery_capacity 38\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 1\ngrid_power_import 76\ngrid_power_export 76\ngenset_polynom_order 3\ngenset_polynom_0 1.2718118921688693\ngenset_polynom_1 0.3985592241441276\ngenset_polynom_2 0.01701526411445169\ngenset_rated_power 43\ngenset_pmin 0.05\ngenset_pmax 0.9\nfuel_cost 0.4\ngenset_co2 2\nepisode: 0 \tscore: -32101.85419434036 \tmemory length: 5846 \tepsilon: 1.0 \tlearning rate: 0.001\nSimilarity to perfect score 3.091875208562272\nepisode: 1 \tscore: -31237.36119935251 \tmemory length: 11692 \tepsilon: 0.918831875421614 \tlearning rate: 0.001\nSimilarity to perfect score 3.035541656822769\nepisode: 2 \tscore: -29337.07421601616 \tmemory length: 17538 \tepsilon: 0.685943906147323 \tlearning rate: 0.001\nSimilarity to perfect score 2.9117119488710173\n905.137352251\n"
    }
   ],
   "source": [
    "perfect_train_scores = [4068.5, 13568.92, 15345.97]\n",
    "similarity_stop = 0.2\n",
    "MAX_EPISODES = 3\n",
    "\n",
    "def train(building_idx, building_env, perfect_train_score):\n",
    "    env = NormalizedMicroGridEnv(building_env)\n",
    "    #env = FlattenObservation(FrameStack(env, 24))\n",
    "\n",
    "    #writer = SummaryWriter(comment=current_time)\n",
    "\n",
    "    state_size = env.observation_space.low.size\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scores, steps = [], 0\n",
    "    scores_list = deque(maxlen=5) #Â yearly score\n",
    "\n",
    "    for e in range(MAX_EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            if agent.memory.tree.n_entries >= agent.train_start:\n",
    "                loss = agent.train_model()\n",
    "                #writer.add_scalar('Loss', loss, steps)\n",
    "                steps += 1\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "                agent.scheduler.step()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                scores.append(score)\n",
    "                print(\"episode:\", e, \"\\tscore:\", score, \"\\tmemory length:\",\n",
    "                        agent.memory.tree.n_entries, \"\\tepsilon:\", agent.epsilon,\n",
    "                        \"\\tlearning rate:\", agent.scheduler.get_last_lr()[0])\n",
    "\n",
    "                #writer.add_scalar('Learning rate', agent.scheduler.get_last_lr()[0], e)\n",
    "                #writer.add_scalar('Training total building cost', score, e)\n",
    "\n",
    "                torch.save(agent.model, \"./save_model/per_dqn_\" + str(building_idx))\n",
    "                torch.save(agent.model, \"./save_model/per_dqn_\" +  str(building_idx) + \"_\" +\n",
    "                            current_time + \"_\" + str(e).zfill(5))\n",
    "\n",
    "                # Early stop\n",
    "                similarity = abs(score - perfect_train_score) / perfect_train_score\n",
    "                print(\"Similarity to perfect score\", similarity)\n",
    "                if similarity <= similarity_stop:\n",
    "                    print(\"Reached similarity stop of\", similarity_stop)\n",
    "                    return\n",
    "                if len(scores_list) >= scores_list.maxlen:\n",
    "                    mean = np.mean(scores_list)\n",
    "                    similarity = abs(score - mean) / abs(mean)\n",
    "                    print(\"Similarity to stable score\", similarity)\n",
    "                    if similarity <= similarity_stop:\n",
    "                        print(\"Reached stable score\")\n",
    "                        return\n",
    "                scores_list.append(score)\n",
    "\n",
    "train_start = time.process_time()\n",
    "\n",
    "for building_idx, (building_env, perfect_train_score) in enumerate(zip(building_environments, perfect_train_scores)):\n",
    "    train(building_idx, building_env, perfect_train_score)\n",
    "\n",
    "train_end = time.process_time()\n",
    "train_frugality = train_end - train_start\n",
    "print(train_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Test of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load 15\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 30\nbattery_soc_0 0.2\nbattery_power_charge 4\nbattery_power_discharge 4\nbattery_capacity 15\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 0\ngrid_power_import 30\ngrid_power_export 30\nload 50\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 75\nbattery_soc_0 0.2\nbattery_power_charge 13\nbattery_power_discharge 13\nbattery_capacity 50\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 0\ngrid_power_import 100\ngrid_power_export 100\nload 38\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 60\nbattery_soc_0 0.2\nbattery_power_charge 10\nbattery_power_discharge 10\nbattery_capacity 38\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 1\ngrid_power_import 76\ngrid_power_export 76\ngenset_polynom_order 3\ngenset_polynom_0 1.2718118921688693\ngenset_polynom_1 0.3985592241441276\ngenset_polynom_2 0.01701526411445169\ngenset_rated_power 43\ngenset_pmin 0.05\ngenset_pmax 0.9\nfuel_cost 0.4\ngenset_co2 2\n6.604903763999914\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Below is an example for a Random Agent \n",
    "\n",
    "Note :\n",
    "* To make your work as reproductible as possible, have a full-greedy approach (no exploration) on the test buildings\n",
    "* If your algorithm has some unavoidable randomness, consider running it for many loops and return a\n",
    "  mean profitability and mean frugality\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i], \"testing\": True}) for i in range(3)]\n",
    "\n",
    "for i, building_env in enumerate(building_environments):\n",
    "    test_done = False\n",
    "    test_score = 0\n",
    "\n",
    "    test_env = NormalizedMicroGridEnv(building_env)\n",
    "    state_size = test_env.observation_space.low.size\n",
    "    action_size = test_env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.model = torch.load('save_model/per_dqn_' + str(i))\n",
    "    agent.model.train(False)\n",
    "    \n",
    "    state = test_env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not test_done:\n",
    "            state = torch.from_numpy(state).float().cpu()\n",
    "            q_value = agent.model(state)\n",
    "            _, action = torch.max(q_value, 1)\n",
    "            action = int(action)\n",
    "            next_state, reward, test_done, info = test_env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            test_score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if test_done:\n",
    "                #writer.add_scalar('Testing total building cost', test_score, e)\n",
    "                total_cost[i] = test_score\n",
    "\n",
    "test_end = time.process_time()\n",
    "\n",
    "test_frugality = test_end - test_start\n",
    "print(test_frugality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "911.742256015\n"
    }
   ],
   "source": [
    "frugality = train_frugality + test_frugality\n",
    "print(frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'building_1_performance': -4089.2658357035007, 'building_2_performance': -13630.886119011686, 'building_3_performance': -16741.52610708305, 'frugality': 911.742256015}\n"
    }
   ],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0],\n",
    "    \"building_2_performance\" : total_cost[1],\n",
    "    \"building_3_performance\" : total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = \"team17\"\n",
    "import json\n",
    "with open(team_name + '.txt', 'w') as json_file:\n",
    "    json.dump(final_results, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('geometric37': conda)",
   "language": "python",
   "name": "python37964bitgeometric37condafdbadd2b6eaf45748206fa33410bc006"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}