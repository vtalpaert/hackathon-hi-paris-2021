{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Notebook for TEAM 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Install your packages below: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxpy\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, you must run your methodology for solving the problem from start to finish :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    \n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv # Imposed Environment\n",
    "import numpy as np\n",
    "\n",
    "## Import your favourite Deep Learning library for RL and other packages here\n",
    "from train_per import *\n",
    "from wrappers import NormalizedMicroGridEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an environment initialization without a Deep RL library, the code can vary depending on which library you \n",
    "use\n",
    "\"\"\"\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i]}) for i in range(3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Training of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load 15\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 30\nbattery_soc_0 0.2\nbattery_power_charge 4\nbattery_power_discharge 4\nbattery_capacity 15\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 0\ngrid_power_import 30\ngrid_power_export 30\nepisode: 0 \tscore: -12558.37539653331 \tmemory length: 5846 \tepsilon: 1.0 \tlearning rate: 0.001\nSimilarity to perfect score 4.086733537306946\nepisode: 1 \tscore: -12700.626630075618 \tmemory length: 11692 \tepsilon: 0.918831875421614 \tlearning rate: 0.001\nSimilarity to perfect score 4.1216975863526155\nepisode: 2 \tscore: -10305.16148007997 \tmemory length: 17538 \tepsilon: 0.685943906147323 \tlearning rate: 0.001\nSimilarity to perfect score 3.532914214103471\nepisode: 3 \tscore: -8962.665524180467 \tmemory length: 23384 \tepsilon: 0.5120839350123164 \tlearning rate: 0.001\nSimilarity to perfect score 3.2029410161436567\nepisode: 4 \tscore: -7982.150120028236 \tmemory length: 29230 \tepsilon: 0.38229067150773627 \tlearning rate: 0.001\nSimilarity to perfect score 2.9619393191663357\nepisode: 5 \tscore: -7238.616670986087 \tmemory length: 32768 \tepsilon: 0.28539492752945217 \tlearning rate: 0.001\nSimilarity to perfect score 2.7791856141049744\nSimilarity to stable score 0.3107258236554054\nepisode: 6 \tscore: -6832.206877589275 \tmemory length: 32768 \tepsilon: 0.21305846762701416 \tlearning rate: 0.001\nSimilarity to perfect score 2.6792938128522246\nSimilarity to stable score 0.2760839428999164\nepisode: 7 \tscore: -6343.510435592098 \tmemory length: 32768 \tepsilon: 0.1590564731494293 \tlearning rate: 0.001\nSimilarity to perfect score 2.5591767077773375\nSimilarity to stable score 0.23240712518937576\nepisode: 8 \tscore: -6140.159077595429 \tmemory length: 32768 \tepsilon: 0.11874187368616633 \tlearning rate: 0.001\nSimilarity to perfect score 2.5091948083066065\nSimilarity to stable score 0.1782255299339486\nReached stable score\nload 50\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 75\nbattery_soc_0 0.2\nbattery_power_charge 13\nbattery_power_discharge 13\nbattery_capacity 50\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 0\ngrid_power_import 100\ngrid_power_export 100\nepisode: 0 \tscore: -41528.19602943267 \tmemory length: 5846 \tepsilon: 1.0 \tlearning rate: 0.001\nSimilarity to perfect score 4.06053805530821\nepisode: 1 \tscore: -42027.80205872475 \tmemory length: 11692 \tepsilon: 0.918831875421614 \tlearning rate: 0.001\nSimilarity to perfect score 4.097357937015234\nepisode: 2 \tscore: -34370.3444382345 \tmemory length: 17538 \tepsilon: 0.685943906147323 \tlearning rate: 0.001\nSimilarity to perfect score 3.5330199041806196\nepisode: 3 \tscore: -29648.725953332458 \tmemory length: 23384 \tepsilon: 0.5120839350123164 \tlearning rate: 0.001\nSimilarity to perfect score 3.185046853642918\nepisode: 4 \tscore: -26156.119812749457 \tmemory length: 29230 \tepsilon: 0.38229067150773627 \tlearning rate: 0.001\nSimilarity to perfect score 2.9276493495981595\nepisode: 5 \tscore: -24064.431346880614 \tmemory length: 32768 \tepsilon: 0.28539492752945217 \tlearning rate: 0.001\nSimilarity to perfect score 2.7734964423757096\nSimilarity to stable score 0.30742339405494357\nepisode: 6 \tscore: -22318.7734576798 \tmemory length: 32768 \tepsilon: 0.21305846762701416 \tlearning rate: 0.001\nSimilarity to perfect score 2.6448452388015995\nSimilarity to stable score 0.2858788817881704\nepisode: 7 \tscore: -21393.679014199977 \tmemory length: 32768 \tepsilon: 0.1590564731494293 \tlearning rate: 0.001\nSimilarity to perfect score 2.576667783007047\nSimilarity to stable score 0.216683858476467\nepisode: 8 \tscore: -19876.332699484832 \tmemory length: 32768 \tepsilon: 0.11874187368616633 \tlearning rate: 0.001\nSimilarity to perfect score 2.4648426477188186\nSimilarity to stable score 0.1958223611913776\nReached stable score\nload 38\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 60\nbattery_soc_0 0.2\nbattery_power_charge 10\nbattery_power_discharge 10\nbattery_capacity 38\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 1\ngrid_power_import 76\ngrid_power_export 76\ngenset_polynom_order 3\ngenset_polynom_0 1.2718118921688693\ngenset_polynom_1 0.3985592241441276\ngenset_polynom_2 0.01701526411445169\ngenset_rated_power 43\ngenset_pmin 0.05\ngenset_pmax 0.9\nfuel_cost 0.4\ngenset_co2 2\nepisode: 0 \tscore: -32070.1445339773 \tmemory length: 5846 \tepsilon: 1.0 \tlearning rate: 0.001\nSimilarity to perfect score 3.0898088901501373\nepisode: 1 \tscore: -32087.004153678598 \tmemory length: 11692 \tepsilon: 0.918831875421614 \tlearning rate: 0.001\nSimilarity to perfect score 3.0909075251469016\nepisode: 2 \tscore: -29269.010908069038 \tmemory length: 17538 \tepsilon: 0.685943906147323 \tlearning rate: 0.001\nSimilarity to perfect score 2.9072766927127476\nepisode: 3 \tscore: -27265.438847619604 \tmemory length: 23384 \tepsilon: 0.5120839350123164 \tlearning rate: 0.001\nSimilarity to perfect score 2.776716548228597\nepisode: 4 \tscore: -25449.726497687443 \tmemory length: 29230 \tepsilon: 0.38229067150773627 \tlearning rate: 0.001\nSimilarity to perfect score 2.658398035294442\nepisode: 5 \tscore: -24100.539129237426 \tmemory length: 32768 \tepsilon: 0.28539492752945217 \tlearning rate: 0.001\nSimilarity to perfect score 2.570480010663218\nSimilarity to stable score 0.17543723040139425\nReached stable score\n3642.170199235001\n"
    }
   ],
   "source": [
    "perfect_train_scores = [4068.5, 13568.92, 15345.97]\n",
    "similarity_stop = 0.2\n",
    "MAX_EPISODES = 10\n",
    "\n",
    "def train(building_idx, building_env, perfect_train_score):\n",
    "    env = NormalizedMicroGridEnv(building_env)\n",
    "    #env = FlattenObservation(FrameStack(env, 24))\n",
    "\n",
    "    #writer = SummaryWriter(comment=current_time)\n",
    "\n",
    "    state_size = env.observation_space.low.size\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scores, steps = [], 0\n",
    "    scores_list = deque(maxlen=5) #Â yearly score\n",
    "\n",
    "    for e in range(MAX_EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            if agent.memory.tree.n_entries >= agent.train_start:\n",
    "                loss = agent.train_model()\n",
    "                #writer.add_scalar('Loss', loss, steps)\n",
    "                steps += 1\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "                agent.scheduler.step()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                scores.append(score)\n",
    "                print(\"episode:\", e, \"\\tscore:\", score, \"\\tmemory length:\",\n",
    "                        agent.memory.tree.n_entries, \"\\tepsilon:\", agent.epsilon,\n",
    "                        \"\\tlearning rate:\", agent.scheduler.get_last_lr()[0])\n",
    "\n",
    "                #writer.add_scalar('Learning rate', agent.scheduler.get_last_lr()[0], e)\n",
    "                #writer.add_scalar('Training total building cost', score, e)\n",
    "\n",
    "                torch.save(agent.model, \"./save_model/per_dqn_\" + str(building_idx))\n",
    "                torch.save(agent.model, \"./save_model/per_dqn_\" +  str(building_idx) + \"_\" +\n",
    "                            current_time + \"_\" + str(e).zfill(5))\n",
    "\n",
    "                # Early stop\n",
    "                similarity = abs(score - perfect_train_score) / perfect_train_score\n",
    "                print(\"Similarity to perfect score\", similarity)\n",
    "                if similarity <= similarity_stop:\n",
    "                    print(\"Reached similarity stop of\", similarity_stop)\n",
    "                    return\n",
    "                if len(scores_list) >= scores_list.maxlen:\n",
    "                    mean = np.mean(scores_list)\n",
    "                    similarity = abs(score - mean) / abs(mean)\n",
    "                    print(\"Similarity to stable score\", similarity)\n",
    "                    if similarity <= similarity_stop:\n",
    "                        print(\"Reached stable score\")\n",
    "                        return\n",
    "                scores_list.append(score)\n",
    "\n",
    "train_start = time.process_time()\n",
    "\n",
    "for building_idx, (building_env, perfect_train_score) in enumerate(zip(building_environments, perfect_train_scores)):\n",
    "    train(building_idx, building_env, perfect_train_score)\n",
    "\n",
    "train_end = time.process_time()\n",
    "train_frugality = train_end - train_start\n",
    "print(train_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Test of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load 15\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 30\nbattery_soc_0 0.2\nbattery_power_charge 4\nbattery_power_discharge 4\nbattery_capacity 15\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 0\ngrid_power_import 30\ngrid_power_export 30\nload 50\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 75\nbattery_soc_0 0.2\nbattery_power_charge 13\nbattery_power_discharge 13\nbattery_capacity 50\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 0\ngrid_power_import 100\ngrid_power_export 100\nload 38\ncost_loss_load 1\ncost_overgeneration 0.1\ncost_co2 0.25\nPV_rated_power 60\nbattery_soc_0 0.2\nbattery_power_charge 10\nbattery_power_discharge 10\nbattery_capacity 38\nbattery_efficiency 0.9\nbattery_soc_min 0.2\nbattery_soc_max 1\nbattery_cost_cycle 0.02\ngrid_weak 1\ngrid_power_import 76\ngrid_power_export 76\ngenset_polynom_order 3\ngenset_polynom_0 1.2718118921688693\ngenset_polynom_1 0.3985592241441276\ngenset_polynom_2 0.01701526411445169\ngenset_rated_power 43\ngenset_pmin 0.05\ngenset_pmax 0.9\nfuel_cost 0.4\ngenset_co2 2\n5.037817084999915\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Below is an example for a Random Agent \n",
    "\n",
    "Note :\n",
    "* To make your work as reproductible as possible, have a full-greedy approach (no exploration) on the test buildings\n",
    "* If your algorithm has some unavoidable randomness, consider running it for many loops and return a\n",
    "  mean profitability and mean frugality\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i], \"testing\": True}) for i in range(3)]\n",
    "\n",
    "for i, building_env in enumerate(building_environments):\n",
    "    test_done = False\n",
    "    test_score = 0\n",
    "\n",
    "    test_env = NormalizedMicroGridEnv(building_env)\n",
    "    state_size = test_env.observation_space.low.size\n",
    "    action_size = test_env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.model = torch.load('save_model/per_dqn_' + str(i))\n",
    "    agent.model.train(False)\n",
    "    \n",
    "    state = test_env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not test_done:\n",
    "            state = torch.from_numpy(state).float().cpu()\n",
    "            q_value = agent.model(state)\n",
    "            _, action = torch.max(q_value, 1)\n",
    "            action = int(action)\n",
    "            next_state, reward, test_done, info = test_env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            test_score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if test_done:\n",
    "                #writer.add_scalar('Testing total building cost', test_score, e)\n",
    "                total_cost[i] = test_score\n",
    "\n",
    "test_end = time.process_time()\n",
    "\n",
    "test_frugality = test_end - test_start\n",
    "print(test_frugality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3078.243593267\n"
    }
   ],
   "source": [
    "frugality = train_frugality + test_frugality\n",
    "print(frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'building_1_performance': -4192.850478697825, 'building_2_performance': -13630.886119011686, 'building_3_performance': -16480.84413419341, 'frugality': 3078.243593267}\n"
    }
   ],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0],\n",
    "    \"building_2_performance\" : total_cost[1],\n",
    "    \"building_3_performance\" : total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = \"team17\"\n",
    "import json\n",
    "with open(team_name + '.txt', 'w') as json_file:\n",
    "    json.dump(final_results, json_file)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('geometric37': conda)",
   "language": "python",
   "name": "python37964bitgeometric37condafdbadd2b6eaf45748206fa33410bc006"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}